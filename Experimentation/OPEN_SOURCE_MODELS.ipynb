{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#OPEN-SOURCE MODEL TESTING\n",
        "This notebook tests the various open-source models on a custom dataset using RAG Prompting and Zero-Shot Prompting"
      ],
      "metadata": {
        "id": "VSaDB9rl3BOY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##How to run\n",
        "Run the sections:\n",
        "\n",
        " - Import\n",
        "\n",
        " - Data Loading\n",
        "\n",
        " - Retrieval Model\n",
        "\n",
        " - Functions\n",
        "\n",
        "To import the required dependencies and load the query_data.Load the retrieval model. Run the functions for zero-shot prompts and rag prompts.\n",
        "\n",
        "Test the model of your choice\n",
        "\n"
      ],
      "metadata": {
        "id": "75yFdBOArlLR"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7q7Oj5tkKDD"
      },
      "source": [
        "# Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d6brJdNj878J"
      },
      "outputs": [],
      "source": [
        "!pip install openai faiss-cpu sentence-transformers ctransformers[cuda]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sxezHUV7-R_y"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import openai\n",
        "import tqdm\n",
        "from transformers import AutoModel\n",
        "from numpy.linalg import norm\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "import faiss\n",
        "import tqdm\n",
        "import time\n",
        "from ctransformers import AutoModelForCausalLM\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxa52UXbkOrR"
      },
      "source": [
        "# Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NFWFYUBp3ePw"
      },
      "outputs": [],
      "source": [
        "# !wget --no-check-certificate 'https://drive.google.com/uc?export=download&id=1kkkmxgPF0EHfBLx79Epyo4STdiA-HmRX' -O examples.json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json"
      ],
      "metadata": {
        "id": "m-RbT8lzV0Dp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "26iQNnIf-Y3f"
      },
      "outputs": [],
      "source": [
        "!wget -O tools.json \"https://drive.usercontent.google.com/download?id=1RSC_94KKmvtWnI15RVZkLCy4eTVhxGU_&export=download&authuser=0&confirm=t&uuid=394a0e2f-bb02-4027-8224-ef451c182f49&at=APZUnTVfd2focJ6u3DYRiDeKWBUr:1702403341998\"\n",
        "!wget -O examples.json \"https://drive.usercontent.google.com/download?id=1n9eede9tNiqfPfWyc3lx1AHgqJgxLFQ5&export=download&authuser=0&confirm=t&uuid=eabc4504-aca7-4dfc-a6d9-52d357149ee6&at=APZUnTWmVVzr0utprDNGk8VXTU6f:1702403392202\"\n",
        "!wget -O claude_examples_ps_tools_all.json \"https://drive.usercontent.google.com/download?id=17orzUW3_n31pZFyL0F-ekGH5iKlh2SUX&export=download&authuser=0&confirm=t&uuid=61097f63-24fe-4c51-9710-77d7fd204c2b&at=APZUnTXUxVA_067GuWmlaIrm74Io:1702403397617\"\n",
        "with open('/content/examples.json', 'r') as f:\n",
        "    query_data = json.load(f)\n",
        "\n",
        "with open('/content/tools.json', 'r') as f:\n",
        "    tool_data = json.load(f)\n",
        "\n",
        "with open('/content/claude_examples_ps_tools_all.json', 'r') as f:\n",
        "    tool_query_data = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query_1 = \"Summarize my P1 issues in triage\"\n",
        "query_2 = \"Prioritize tasks similar to deve/0:issue/5\"\n",
        "query_3 = \"Add my blocker tickets to the current sprint\"\n",
        "query_4 = \"Prioritize low tickets that need response\"\n",
        "query_5 = \"Get all issues with priority p0 or p1 assigned to me and summarize them\"\n",
        "query_6 = \"List all tickets from github that need a response and add them to the current sprint\"\n",
        "query_7 = \"Get tasks assigned to me in triage, summarize them and create issues from that\"\n",
        "query_8 = \"List high severity github tickets from customer ABC, summarize them and prioritize the summary\"\n",
        "query_9 = \"Get all blocker tickets assigned to me, summarize and create issues from summary\"\n",
        "query_10 = \"List issues assigned to John, summarize them and prioritize\"\n",
        "query_11 = \"List high priority tickets from Acme Corp, summarize them\"\n",
        "query_12 = \"Add my actionable tasks from meeting notes M to my current sprint\"\n",
        "query_13 = \"What are similar issues to issue ISSUE-1?\"\n",
        "query_14 = \"Summarize my tickets needing response\"\n",
        "query_15 = \"Get me all the blocker tickets from github and slack channels under parts ENH-123, PROD-123, CAPL-123, CAPL-359 that need a response, prioritize them and add them to the current sprint\"\n",
        "query_16 = \"What are all my p0, p1 priority issues under parts HGH-262, FEAT-007 in the design stage? Summarize them and add them to the current sprint\"\n",
        "query_17 = \"Get all issues assigned to current user that are in design stage under parts FEAT-123 and FEAT-456. Filter them by priority p1 and p2. Summarize them and add them to current sprint.\"\n",
        "query_18 = \"Get all tasks assigned to me. Filter tasks in progress stage. Summarize the tasks. Get similar tasks to the top 3 tasks from the summary and add them to the current sprint\"\n",
        "query_19 = \"Search for user DEV-123, prioritize and add their P0 issues to the current sprint\"\n",
        "query_20 = \"Get the status of a build from last night\"\n",
        "query_21 = \"Schedule a meeting with the stakeholders\"\n",
        "\n",
        "queries_list_eval = [\n",
        "    query_1, query_2, query_3, query_4, query_5, query_6, query_7, query_8, query_9,\n",
        "    query_10, query_11, query_12, query_13, query_14, query_15, query_16, query_17,\n",
        "    query_18, query_19, query_20, query_21\n",
        "]\n"
      ],
      "metadata": {
        "id": "G5UvK_RpV2DZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuRf49APkYhr"
      },
      "source": [
        "# Retrieval Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-cevPc9kFLIj"
      },
      "outputs": [],
      "source": [
        "queries=[]\n",
        "for inp in query_data:\n",
        "  queries.append(inp['query'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "queries"
      ],
      "metadata": {
        "id": "04CtcBqS28Wl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9iWK1aRzT8nz"
      },
      "outputs": [],
      "source": [
        "cos_sim = lambda a,b: (a @ b.T) / (norm(a)*norm(b))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VJewEZ9n_DqX"
      },
      "outputs": [],
      "source": [
        "# model = AutoModel.from_pretrained('jinaai/jina-embeddings-v2-base-en', trust_remote_code=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e64fDw5HSa2f"
      },
      "outputs": [],
      "source": [
        "model = SentenceTransformer('sentence-transformers/multi-qa-MiniLM-L6-cos-v1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IxDZJ4rX_n8-"
      },
      "outputs": [],
      "source": [
        "query_embeddings = model.encode(queries)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zd5AzvCNSaw4"
      },
      "outputs": [],
      "source": [
        "d = query_embeddings.shape[1]\n",
        "index = faiss.IndexFlatL2(d)\n",
        "index.add(query_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JiliLNLfjjBP"
      },
      "outputs": [],
      "source": [
        "def get_relevant_queries(user_query,query_data,encoder=model,k=3):\n",
        "  user_query_embedding = model.encode(user_query)\n",
        "  user_query_embedding=np.expand_dims(user_query_embedding,axis=0)\n",
        "  D, I = index.search(user_query_embedding, k)\n",
        "  relevant_queries=[]\n",
        "  for i in range(k):\n",
        "    relevant_queries_index = I[0][i]\n",
        "    relevant_queries.append(query_data[relevant_queries_index])\n",
        "  return relevant_queries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOcJg5_mkrf_"
      },
      "source": [
        "# Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qr7YYZoTcedN"
      },
      "outputs": [],
      "source": [
        "def prompt(relevant_queries,k=3):\n",
        "  l=[]\n",
        "  for i in range(k):\n",
        "    l.append({\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"QUESTION: {relevant_queries[i]['query']}\",\n",
        "            })\n",
        "    l.append({\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": f\"ANSWER: {relevant_queries[i]['answer']}\",\n",
        "            })\n",
        "  return l"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1KI8nDto9MkO"
      },
      "outputs": [],
      "source": [
        "def chat_prompt_simple(tools,query_data, question):\n",
        "    sys_prompt = '''\n",
        "                You are a highly intelligent AI assistant that can help users with questions about a certain wide range of topics involving customer relationship management (CRM). You specialize in writing API calls that are correct, simple, and concise.\n",
        "                {{\n",
        "                \"tools\": {}\n",
        "                }}\n",
        "                EXAMPLES\n",
        "              '''.format(tools)\n",
        "    return [\n",
        "          {\n",
        "\n",
        "              \"role\": \"system\",\n",
        "              \"content\": sys_prompt\n",
        "\n",
        "          },\n",
        "          {\n",
        "              \"role\": \"user\",\n",
        "              \"content\": \"QUESTION: What are all my issues in the triage stage under part FEAT-123? Summarize them.\",\n",
        "          },\n",
        "          {\n",
        "              \"role\": \"assistant\",\n",
        "              \"content\":\n",
        "              \"\"\"\n",
        "              {\n",
        "              'REASONING': 'First, we use the \"whoami\" tool to get the user id of the person asking the query. Then, we use the \"works\" list tool to figure out the work items assigned to the user. We pass  $$PREV[0] to get the previous tool call's output as the \"owned_by\" argument. We pass the \"stage_name\" argument as \"triage\" to get issues in the triage stage, and \"applies_to_part\" argument as \"FEAT-123\" to filter out the relevant parts. We then use the \"summarize_objects\" tool with $$PREV[1] as its argument to summarize the outputs of the last tool call.'\n",
        "              'ANSWER:':\n",
        "              [\n",
        "              {\n",
        "              \"tool_name\": \"whoami\",\n",
        "              \"arguments\": []\n",
        "              },\n",
        "              {\n",
        "              \"tool_name\": \"works_list\",\n",
        "              \"arguments\": [\n",
        "              {\n",
        "              \"argument_name\": \"stage.name\",\n",
        "              \"argument_value\": \"triage\"\n",
        "              },\n",
        "              {\n",
        "              \"argument_name\": \"applies_to_part\",\n",
        "              \"argument_value\": \"FEAT-123\"\n",
        "              },\n",
        "              {\n",
        "              \"argument_name\": \"owned_by\",\n",
        "              \"argument_value\": \"$$PREV[0]\"\n",
        "              }\n",
        "              ]\n",
        "              },\n",
        "              {\n",
        "              \"tool_name\": \"summarize_objects\",\n",
        "              \"arguments\": [\n",
        "              {\n",
        "              \"argument_name\": \"objects\",\n",
        "              \"argument_value\": \"$$PREV[1]\"\n",
        "              }\n",
        "              ]\n",
        "              }\n",
        "              ]\n",
        "              }\n",
        "              \"\"\"\n",
        "          },\n",
        "          {\n",
        "              \"role\": \"user\",\n",
        "              \"content\": \"QUESTION: Summarize high severity tickets from the customer UltimateCustomer\"\n",
        "          },\n",
        "          {\n",
        "              \"role\": \"assistant\",\n",
        "              \"content\": \"\"\"\n",
        "              {'REASONING' : 'First, we use the \"search_objects_by_name\" tool to search for the exact user, and filter them out by passing the query as \"UltimateCustomer\", to only get the user id of 'UltimateCustomer'. We then search for the issues created by the customer by using the \"works_list\" tool with the previous output by passing $$PREV[0] as 'ticket.rev_org' to filter out the issues by the customer, and pass the 'ticket.severity' arg as 'high' .We then use the \"summarize_objects\" tool to finally summarize these issues, by passing in the outputs of the previous tool call as $$PREV[1].'\n",
        "              'ANSWER' :\n",
        "              [\n",
        "              {\n",
        "              \"tool_name\": \"search_object_by_name\",\n",
        "              \"arguments\": [\n",
        "              {\n",
        "              \"argument_name\": \"query\",\n",
        "              \"argument_value\": \"UltimateCustomer\"\n",
        "              }\n",
        "              ]\n",
        "              },\n",
        "              {\n",
        "              \"tool_name\": \"works_list\",\n",
        "              \"arguments\": [\n",
        "              {\n",
        "              \"argument_name\": \"ticket.rev_org\",\n",
        "              \"argument_value\": \"$$PREV[0]\"\n",
        "              },\n",
        "              {\n",
        "              \"argument_name\": \"ticket.severity\",\n",
        "              \"argument_value\": \"[\"high\"]\"\n",
        "              }\n",
        "              ]\n",
        "              },\n",
        "              {\n",
        "              \"tool_name\": \"summarize_objects\",\n",
        "              \"arguments\": [\n",
        "              {\n",
        "              \"argument_name\": \"objects\",\n",
        "              \"argument_value\": \"$$PREV[1]\"\n",
        "              }\n",
        "              ]\n",
        "              }\n",
        "              ]\n",
        "              }\n",
        "              \"\"\"},\n",
        "      {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"QUESTION: Get all work items similar to TKT-123, summarize them, create issues from that summary, and prioritize them.\"\n",
        "      },\n",
        "      {\n",
        "        \"role\": \"assistant\",\n",
        "        \"content\": \"\"\"\n",
        "        {\n",
        "        'REASONING': 'We use the tool \"get_similar_work_items\" to get work items similar to \"TKT-123\". We then summarize the outputs with the \"summarize_objects\" tool, passing the previous tool's outputs in the 'objects' argument as $$PREV[0]. We then use \"create_actionable_tasks_from_text\" on the outputs of the previous tool call with $$PREV[1], as the previous tool call returned the output numbered 1,  to generate issues from the summarized work items. Finally, we use the \"prioritize_objects\" tool with the argument 'objects' passed as $$PREV[2]  to prioritize the outputs of the last tool call.'\n",
        "        'ANSWER': [\n",
        "        {\n",
        "        \"tool_name\": \"get_similar_work_items\",\n",
        "        \"arguments\": [\n",
        "        {\n",
        "        \"argument_name\": \"work_id\",\n",
        "        \"argument_value\": \"TKT-123\"\n",
        "        }\n",
        "        ]\n",
        "        },\n",
        "        {\n",
        "        \"tool_name\": \"summarize_objects\",\n",
        "        \"arguments\": [\n",
        "        {\n",
        "        \"argument_name\": \"objects\",\n",
        "        \"argument_value\": \"$$PREV[0]\"\n",
        "        }\n",
        "        ]\n",
        "        },\n",
        "        {\n",
        "        \"tool_name\": \"create_actionable_tasks_from_text\",\n",
        "        \"arguments\": [\n",
        "        {\n",
        "        \"argument_name\": \"text\",\n",
        "        \"argument_value\": \"$$PREV[1]\"\n",
        "        }\n",
        "        ]\n",
        "        },\n",
        "        {\n",
        "        \"tool_name\": \"prioritize_objects\",\n",
        "        \"arguments\": [\n",
        "        {\n",
        "        \"argument_name\": \"objects\",\n",
        "        \"argument_value\": \"$$PREV[2]\"\n",
        "        }\n",
        "        ]\n",
        "        }\n",
        "        ]\n",
        "        }\n",
        "        \"\"\"},\n",
        "      {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"Now, it is your task to answer the following query\"\n",
        "      },\n",
        "      {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": f\"QUESTION: {question}\"\n",
        "      }\n",
        "      ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j5bTFN-0U_7J"
      },
      "outputs": [],
      "source": [
        "def chat_prompt_with_retrieval(tools,query_data,question):\n",
        "    sys_prompt = '''\n",
        "                You are a highly intelligent AI assistant that can help users with questions about a certain wide range of topics involving customer relationship management (CRM). You specialize in writing API calls that are correct, simple, and concise.\n",
        "                {{\n",
        "                \"tools\": {}\n",
        "                }}\n",
        "                EXAMPLES\n",
        "              '''.format(tools)\n",
        "    relevant_queries=get_relevant_queries(question,query_data)\n",
        "    return [\n",
        "          {\n",
        "              \"role\": \"system\",\n",
        "              \"content\": sys_prompt,\n",
        "          },\n",
        "          {\n",
        "              \"role\": \"user\",\n",
        "              \"content\": f\"QUESTION: {relevant_queries[0]['query']}\",\n",
        "          },\n",
        "          {\n",
        "              \"role\": \"assistant\",\n",
        "              \"content\": f\"{relevant_queries[0]['answer']}\",\n",
        "          },\n",
        "          {\n",
        "              \"role\": \"user\",\n",
        "              \"content\": f\"QUESTION: {relevant_queries[1]['query']}\",\n",
        "          },\n",
        "          {\n",
        "              \"role\": \"assistant\",\n",
        "              \"content\":f\"{relevant_queries[1]['answer']}\",\n",
        "          },\n",
        "          {\n",
        "              \"role\": \"user\",\n",
        "              \"content\": f\"QUESTION: {relevant_queries[2]['query']}\",\n",
        "          },\n",
        "          {\n",
        "              \"role\": \"assistant\",\n",
        "              \"content\": f\"{relevant_queries[2]['answer']}\",\n",
        "          },\n",
        "          {\n",
        "              \"role\": \"system\",\n",
        "              \"content\": \"Now, it is your task to answer the following query and the answer should be in the same format as mentioned answer above and it should be on json and solve answer step by step with reasoning and no need to give your reasoning while giving the answer. The output should be in json format and use double quotes (\"\") instead of single quotes ('')\"\n",
        "          },\n",
        "          {\n",
        "              \"role\": \"user\",\n",
        "              \"content\": f\"QUESTION: {question}\"\n",
        "          }\n",
        "      ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vCcw5nwzIV_e"
      },
      "outputs": [],
      "source": [
        "def retrieve_prompt_with_model(tool_data,query_data,query,sys_start,sys_end,usr_start,usr_end,ast_start):\n",
        "  relevant_queries=get_relevant_queries(query,query_data)\n",
        "  prompt_template=f\"\"\"{sys_start} You are a highly intelligent AI assistant that can help users with questions that can be answered by the use of a set of TOOLS. You specialize in identifying tools with arguments to sove the query.\n",
        "  You are given the following TOOLS with their DESCRIPTIONS, ARGUMENTS to the tool in question with their DESCRIPTIONS.\n",
        "  {tool_data}.\n",
        "   To reference the value of the ith tool in the chain, use $$PREV[i] as argument value, where i refers to the output of the tool positioned in the ith place in the JSON.\n",
        "   EXAMPLE QUERY 1: {relevant_queries[0]['query']}\n",
        "   ANSWER: {relevant_queries[0]['answer']}\n",
        "   EXAMPLE QUERY 2: {relevant_queries[1]['query']}\n",
        "   ANSWER: {relevant_queries[1]['answer']}\n",
        "   EXAMPLE QUERY 3: {relevant_queries[2]['query']}\n",
        "   ANSWER: {relevant_queries[2]['answer']}\n",
        "  The above are solutions to some queries.\n",
        "   {sys_end}. {usr_start} It is your task to answer the following QUERY. Think step by step. ONLY OUTPUT THE ANSWER TO THE QUERY AND THE ANSWER SHOULD BE IN JSON FORMAT. DO NOT DEVIATE from the instructions and keep in mind that the output answer should be in correct json format.\n",
        "   QUERY: {query}. {usr_end} {ast_start}\"\"\"\n",
        "  return prompt_template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mVhkT9fuHc9I"
      },
      "outputs": [],
      "source": [
        "def predict(tool_data,query_data,test_query,llm,temperature,sys_start,sys_end,usr_start,usr_end,ast_start):\n",
        "  return llm(retrieve_prompt_with_model(tool_data,query_data,test_query,sys_start,sys_end,usr_start,usr_end,ast_start))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_json(pred):\n",
        "  return json.loads(pred[pred.find('{'):-1*(\"\".join(reversed(pred)).find('}')+1)] + '}')"
      ],
      "metadata": {
        "id": "jgi8N4BM_a5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def postprocess(pred, tool_data):\n",
        "  json_pred = get_json(pred)\n",
        "  tools = {}\n",
        "  for i,tool in enumerate(tool_data[\"tools\"]):\n",
        "    tools[tool[\"tool_name\"]] = tool\n",
        "    tools[tool[\"tool_name\"]][\"args\"] = {}\n",
        "    for arg in tool[\"argument_list\"]:\n",
        "      tools[tool[\"tool_name\"]][\"args\"][arg[\"argument_name\"]] = arg\n",
        "  for tool in json_pred:\n",
        "    for j, arg in enumerate(tool[\"argument_list\"]):\n",
        "      arg_type = tools[tool[\"tool_name\"]][\"args\"][arg[\"argument_name\"]][\"argument_type\"]\n",
        "      temp = json_pred[i][\"argument_list\"][j][\"argument_value\"]\n",
        "      if arg_type == \"string\":\n",
        "        temp = str(temp)\n",
        "      elif arg_type == \"array of strings\" or arg_type == \"array of objects\":\n",
        "        temp = [str(temp)]\n",
        "      json_pred[i][\"argument_list\"][j][\"argument_value\"] = temp\n",
        "  return json_pred"
      ],
      "metadata": {
        "id": "bIFxWuvb_cFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#RAG Prompting"
      ],
      "metadata": {
        "id": "XgcBU_jct5QF"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qjLJfNPWs2E"
      },
      "source": [
        "##Zephyr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UTHQtUBwXUQ7"
      },
      "outputs": [],
      "source": [
        "zephyr = AutoModelForCausalLM.from_pretrained(\"TheBloke/zephyr-7B-beta-GGUF\", model_file=\"zephyr-7b-beta.Q4_K_M.gguf\", model_type='mistral',gpu_layers=400,context_length=8192,max_new_tokens=2048,temperature=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9F8ZzfbsYGID"
      },
      "outputs": [],
      "source": [
        "response_zephyr=[]\n",
        "for query in tqdm.tqdm(queries):\n",
        "  a=predict(tool_data,query_data,query,zephyr,0.5,'<|system|>','</s>','<|user|>','</s>','<|assistant|>')\n",
        "  response_zephyr.append({'query':query,'answer':a})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uUluucqiYVsn"
      },
      "outputs": [],
      "source": [
        "response_zephyr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_Q0C-qNacLA"
      },
      "source": [
        "## Mistral"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m2erm9faZdgi"
      },
      "outputs": [],
      "source": [
        "mistral = AutoModelForCausalLM.from_pretrained(\"TheBloke/Mistral-7B-Instruct-v0.1-GGUF\", model_file=\"mistral-7b-instruct-v0.1.Q4_K_M.gguf\", model_type=\"mistral\",gpu_layers=900,context_length=8192,max_new_tokens=2048,temperature=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5BcNWRzZZv5R"
      },
      "outputs": [],
      "source": [
        "response_mistral=[]\n",
        "for query in tqdm.tqdm(queries_list_eval):\n",
        "  print(query)\n",
        "  a=predict(tool_data,query_data,query,mistral,0.5,'<s>[INST]',' ',' ','[/INST}</s>','')\n",
        "  response_mistral.append({'query':query,'answer':a})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ojCs5IUUZ0nN"
      },
      "outputs": [],
      "source": [
        "response_mistral"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixWjSbjP323L"
      },
      "source": [
        "## LLama 7B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxHzgQu0323h"
      },
      "outputs": [],
      "source": [
        "llama = AutoModelForCausalLM.from_pretrained(\"TheBloke/Llama-2-7B-GGUF\", model_file=\"llama-2-7b.Q4_K_M.gguf\", model_type=\"llama\",gpu_layers=400,context_length=8192,max_new_tokens=2048,temperature=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MU6NuSCa323i"
      },
      "outputs": [],
      "source": [
        "user_query='Find all the tasks owned by the current user that need a response and are associated with Rev organizations \"REV-123\" and \"REV-456\". Sort them by priority.'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TNqHtBcn323j"
      },
      "outputs": [],
      "source": [
        "pred=predict(tool_data,query_data,user_query,llama,0.5,' ',' ',' ',' ','')\n",
        "pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0yT3Qf4k323j"
      },
      "outputs": [],
      "source": [
        "ans=[]\n",
        "for inp in tqdm.tqdm(test_query_data):\n",
        "  query=inp['query']\n",
        "  a=predict(tool_data,query_data,query,llama,0.5,' ',' ',' ',' ','')\n",
        "  ans.append({'query':query,'answer':a})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NkgGNowk323k"
      },
      "outputs": [],
      "source": [
        "ans"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('llama_rag_dset.pkl','wb') as f:\n",
        "  pickle.dump(ans,f)"
      ],
      "metadata": {
        "id": "vOPaWsXl8ofF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('./drive/MyDrive/Devrev Agent 007/Results/llama_rag_dset.pkl','wb') as f:\n",
        "  pickle.dump(ans,f)"
      ],
      "metadata": {
        "id": "ZyO6hV5h9TuO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKrVOySz6Eyq"
      },
      "source": [
        "## Neural chat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gnoJws8Q6Ey0"
      },
      "outputs": [],
      "source": [
        "neural_chat = AutoModelForCausalLM.from_pretrained(\"TheBloke/neural-chat-7B-v3-1-GGUF\", model_file=\"neural-chat-7b-v3-1.Q4_K_M.gguf\", model_type=\"mistral\",gpu_layers=400,context_length=8192,max_new_tokens=2048,temperature=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eCOkRfsg6Ey0"
      },
      "outputs": [],
      "source": [
        "user_query='Find all the tasks owned by the current user that need a response and are associated with Rev organizations \"REV-123\" and \"REV-456\". Sort them by priority.'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E0gk7v9O6Ey0"
      },
      "outputs": [],
      "source": [
        "pred=predict(tool_data,query_data,user_query,neural_chat,0.5,'### System:',' ',' ','### User:','### Assistant:')\n",
        "pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Z5OusFo6Ey0"
      },
      "outputs": [],
      "source": [
        "ans=[]\n",
        "for inp in tqdm.tqdm(test_query_data):\n",
        "  query=inp['query']\n",
        "  a=predict(tool_data,query_data,query,neural_chat,0.5,'### System:',' ',' ','### User:','### Assistant:')\n",
        "  ans.append({'query':query,'answer':a})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hwEj-b5E6Ey1"
      },
      "outputs": [],
      "source": [
        "ans"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('neural_chat_rag_dset.pkl','wb') as f:\n",
        "  pickle.dump(ans,f)"
      ],
      "metadata": {
        "id": "XTaRzLME9BdJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('./drive/MyDrive/Devrev Agent 007/Results/neural_chat_rag_dset.pkl','wb') as f:\n",
        "  pickle.dump(ans,f)"
      ],
      "metadata": {
        "id": "pBgIUKHw9iZd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kVfb62I6GC6"
      },
      "source": [
        "## Orca mini 7B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YFZEbxtt6GDA"
      },
      "outputs": [],
      "source": [
        "orca = AutoModelForCausalLM.from_pretrained(\"TheBloke/orca_mini_v3_7B-GGUF\",gpu_layers=900,context_length=8192,max_new_tokens=2048,temperature=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cu3hbRZ36GDA"
      },
      "outputs": [],
      "source": [
        "response_orca=[]\n",
        "for query in tqdm.tqdm(queries_list_eval):\n",
        "  print(query)\n",
        "  a=predict(tool_data,query_data,\"### Input: \"+query,orca,0.5,'### System:',' ','### User:',' ','### Response:')\n",
        "  response_orca.append({'query':query,'answer':a})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XbxavIsU6GDB"
      },
      "outputs": [],
      "source": [
        "response_orca"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('orca_7B_rag_dset.pkl','wb') as f:\n",
        "  pickle.dump(ans,f)"
      ],
      "metadata": {
        "id": "rAlPcay09IRe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('./drive/MyDrive/Devrev Agent 007/Results/orca_7B_rag_dset.pkl','wb') as f:\n",
        "  pickle.dump(ans,f)"
      ],
      "metadata": {
        "id": "NMLKIJbz9jCz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## YI-6B"
      ],
      "metadata": {
        "id": "Wj3cDeQk4RTF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "fmZo2KK4xyeJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2FoVy5WAkrIo"
      },
      "outputs": [],
      "source": [
        "yi_6b = AutoModelForCausalLM.from_pretrained(\"TheBloke/Yi-6B-GGUF\", model_file=\"yi-6b.Q5_K_M.gguf\", model_type=\"yi\", gpu_layers=900,context_length=4096)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response_yi6b=[]\n",
        "for query in tqdm.tqdm(queries_list_eval):\n",
        "  print(query)\n",
        "  a=predict(tool_data,query_data,query,yi_6b,0.5,'','','','','')\n",
        "  response_yi6b.append({'query':query,'answer':a})"
      ],
      "metadata": {
        "id": "PRrv_ipL7As1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response_yi6b"
      ],
      "metadata": {
        "id": "u5gdbTOfEAHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "starling = AutoModelForCausalLM.from_pretrained(\"TheBloke/Starling-LM-7B-alpha-GGUF\",model_file = \"starling-lm-7b-alpha.Q5_K_S.gguf\", gpu_layers=400,context_length=4096)"
      ],
      "metadata": {
        "id": "XKzHYbkp6s_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred=predict(tool_data,query_data,user_query,starling,0.5,'','','','','')\n",
        "pred"
      ],
      "metadata": {
        "id": "a5IeZ-f78SAI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ans=[]\n",
        "for inp in tqdm.tqdm(test_query_data):\n",
        "  query=inp['query']\n",
        "  a=predict(tool_data,query_data,\"\"+query,llm,0.5,'','','','','')\n",
        "  ans.append({'query':query,'answer':a})"
      ],
      "metadata": {
        "id": "OH__Cwlp8wEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural Hermes"
      ],
      "metadata": {
        "id": "fHSss0yFJXRM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "neural_hermes = AutoModelForCausalLM.from_pretrained(\"TheBloke/NeuralHermes-2.5-Mistral-7B-GGUF\",model_file = \"neuralhermes-2.5-mistral-7b.Q5_K_M.gguf\", gpu_layers=400,context_length=4096)"
      ],
      "metadata": {
        "id": "V_LHm2J_JW1O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred=predict(tool_data,query_data,user_query,neural_hermes,0.5,'<|im_start|>system','<|im_end|>','<|im_start|>user','<|im_end|>','<|im_start>asisstant')\n",
        "pred"
      ],
      "metadata": {
        "id": "W76SEONgJrVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ans=[]\n",
        "for inp in tqdm.tqdm(test_query_data):\n",
        "  query=inp['query']\n",
        "  a=predict(tool_data,query_data,query,neural_hermes,0.5,'<|im_start|>system','<|im_end|>','<|im_start|>user','<|im_end|>','<|im_start>asisstant')\n",
        "  ans.append({'query':query,'answer':a})\n",
        "\n",
        "ans"
      ],
      "metadata": {
        "id": "VefVI1B4uIOx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Yi-6B-Dragon"
      ],
      "metadata": {
        "id": "CEhLdgIAnp9B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "yi_34b = AutoModelForCausalLM.from_pretrained(\"TheBloke/dragon-yi-6B-v0-GGUF\",model_file = \"dragon-yi-6b-v0.Q5_K_M.gguf\", gpu_layers=400,context_length=4096)"
      ],
      "metadata": {
        "id": "GqZ5BU_sntyu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ans=[]\n",
        "for inp in tqdm.tqdm(test_query_data):\n",
        "  query=inp['query']\n",
        "  a=predict(tool_data,query_data,query,yi_34b,0.3,'','','','','')\n",
        "  ans.append({'query':query,'answer':a})\n",
        "\n",
        "ans"
      ],
      "metadata": {
        "id": "9NnVcpGA1BQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cybertron UNA"
      ],
      "metadata": {
        "id": "L265_gqUSPAH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U bitsandbytes\n",
        "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
        "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
        "!pip install -q -U git+https://github.com/huggingface/accelerate.git"
      ],
      "metadata": {
        "id": "qzkth4U6TGrY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall transformers"
      ],
      "metadata": {
        "id": "PmSJnWTVuJ34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --no-cache-dir transformers sentencepiece"
      ],
      "metadata": {
        "id": "f6GP-cgkSTX9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import bitsandbytes\n",
        "import transformers\n",
        "import json\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "import tqdm\n",
        "import torch"
      ],
      "metadata": {
        "id": "kj9osZm0Tay6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quantization_config = BitsAndBytesConfig(load_in_4bit=True,bnb_4bit_compute_type=torch.float16)"
      ],
      "metadata": {
        "id": "obUvf1pxTpIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " tokenizer = AutoTokenizer.from_pretrained(\"fblgit/una-cybertron-7b-v2-bf16\",trust_remote_code=True)\n",
        " cybertron = AutoModelForCausalLM.from_pretrained(\"fblgit/una-cybertron-7b-v2-bf16\", device_map=\"cuda:0\", trust_remote_code=True,quantization_config=quantization_config).eval()"
      ],
      "metadata": {
        "id": "wSzQ3Ml9TumW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_1 = \"Summarize my P1 issues in triage\"\n",
        "query_2 = \"Prioritize tasks similar to deve/0:issue/5\"\n",
        "query_3 = \"Add my blocker tickets to the current sprint\"\n",
        "query_4 = \"Prioritize low tickets that need response\"\n",
        "query_5 = \"Get all issues with priority p0 or p1 assigned to me and summarize them\"\n",
        "query_6 = \"List all tickets from github that need a response and add them to the current sprint\"\n",
        "query_7 = \"Get tasks assigned to me in triage, summarize them and create issues from that\"\n",
        "query_8 = \"List high severity github tickets from customer ABC, summarize them and prioritize the summary\"\n",
        "query_9 = \"Get all blocker tickets assigned to me, summarize and create issues from summary\"\n",
        "query_10 = \"List issues assigned to John, summarize them and prioritize\"\n",
        "query_11 = \"List high priority tickets from Acme Corp, summarize them\"\n",
        "query_12 = \"Add my actionable tasks from meeting notes M to my current sprint\"\n",
        "query_13 = \"What are similar issues to issue ISSUE-1?\"\n",
        "query_14 = \"Summarize my tickets needing response\"\n",
        "query_15 = \"Get me all the blocker tickets from github and slack channels under parts ENH-123, PROD-123, CAPL-123, CAPL-359 that need a response, prioritize them and add them to the current sprint\"\n",
        "query_16 = \"What are all my p0, p1 priority issues under parts HGH-262, FEAT-007 in the design stage? Summarize them and add them to the current sprint\"\n",
        "query_17 = \"Get all issues assigned to current user that are in design stage under parts FEAT-123 and FEAT-456. Filter them by priority p1 and p2. Summarize them and add them to current sprint.\"\n",
        "query_18 = \"Get all tasks assigned to me. Filter tasks in progress stage. Summarize the tasks. Get similar tasks to the top 3 tasks from the summary and add them to the current sprint\"\n",
        "query_19 = \"Search for user DEV-123, prioritize and add their P0 issues to the current sprint\"\n",
        "query_20 = \"Get the status of a build from last night\"\n",
        "query_21 = \"Schedule a meeting with the stakeholders\"\n",
        "\n",
        "queries = [\n",
        "    query_1, query_2, query_3, query_4, query_5, query_6, query_7, query_8, query_9,\n",
        "    query_10, query_11, query_12, query_13, query_14, query_15, query_16, query_17,\n",
        "    query_18, query_19, query_20, query_21\n",
        "]\n"
      ],
      "metadata": {
        "id": "S2McknqkcQxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pp1=retrieve_prompt_with_model(tool_data,query_data,query_1,'<|im_start|>system','<|im_end|>','<|im_start|>user','<|im_end|>','<|im_start>assistant')"
      ],
      "metadata": {
        "id": "pp8Pu_AzfK2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input=tokenizer(pp1,return_tensors='pt')\n",
        "response = tokenizer.decode(cybertron.generate(**input,max_new_tokens=4096)[0][len(input.input_ids[0]):],skip_special_tokens=True,)"
      ],
      "metadata": {
        "id": "WoxW3GnJT0_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response_simple=[]\n",
        "for query_ in tqdm.tqdm(queries):\n",
        "  pp1 = retrieve_prompt_with_model(tool_data,query_data,query_,'<|im_start|>system','<|im_end|>','<|im_start|>user','<|im_end|>','<|im_start>assistant')\n",
        "  input=tokenizer(pp1,return_tensors='pt')\n",
        "  response = tokenizer.decode(cybertron.generate(**input,max_new_tokens=4096)[0][len(input.input_ids[0]):],skip_special_tokens=True)\n",
        "  response_simple.append(response)"
      ],
      "metadata": {
        "id": "BZP1joNremuk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response_simple"
      ],
      "metadata": {
        "id": "Lqf0Cx7ihHrB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for l in response_simple:\n",
        "  print(l)\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "id": "MS7MZrwvyimX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numba"
      ],
      "metadata": {
        "id": "IseEU8iU0gg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from numba import cuda\n",
        "device = cuda.get_current_device()\n",
        "device.reset()"
      ],
      "metadata": {
        "id": "_pmtRp_twA3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Zero shot Prompting"
      ],
      "metadata": {
        "id": "xAEozUG_eJ6R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Functions"
      ],
      "metadata": {
        "id": "SYAuZJUfAp98"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_prompt_zero_shot(user_query):\n",
        "    message = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You are a intelligent AI agent specialized in giving the tool responses given a dictionary of tools. Here is the dictionary of tools: \"+ json.dumps(tool_data)\n",
        "        }\n",
        "    ]\n",
        "    message.append({\n",
        "        \"role\" : \"user\",\n",
        "        \"content\" : '''\n",
        "        Now its your task to respond to the user queries in the format given below\n",
        "        FORMAT:[{\"tool_name\": \"...\", \"arguments\": [{\"argument_name\": \"...\", \"argument_value\": ... (depending on the argument_type)}, ...]}, ...]\n",
        "        To reference the value of the ith tool in the chain, use $$PREV[i] as argument value. i = 0, 1, .. j-1; j = current tool’s index in the array If the query could not be answered with the given set of tools, output an empty list instead.\n",
        "        Output in the JSON format\n",
        "        '''\n",
        "    })\n",
        "    message.append({\n",
        "        \"role\" : \"user\",\n",
        "        \"content\" : \"Query: \"+ user_query\n",
        "    })\n",
        "    return message"
      ],
      "metadata": {
        "id": "ZPaRnU_rfPFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ex =\"\"\"[{\"tool_name\": \"...\", \"arguments\": [{\"argument_name\": \"...\", \"argument_value\": ... (depending on the argument_type)}, ...]}]\"\"\""
      ],
      "metadata": {
        "id": "awXNfYT97MxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_prompt_with_model(tool_data,query_data,query,sys_start,sys_end,usr_start,usr_end,ast_start):\n",
        "  prompt_template=f\"\"\"{sys_start}You are a intelligent AI agent specialized in giving the tool responses given a dictionary of tools. Here is the dictionary of tools: {tool_data}\n",
        "   {sys_end}. {usr_start} Now its your task to respond to the user queries in the format given below\n",
        "        FORMAT:[{ex}, ...]\n",
        "        To reference the value of the ith tool in the chain, use $$PREV[i] as argument value. i = 0, 1, .. j-1; j = current tool’s index in the array If the query could not be answered with the given set of tools, output an empty list instead.\n",
        "        Output in the JSON format\n",
        "   QUERY: {query}. {usr_end} {ast_start}\"\"\"\n",
        "  return prompt_template"
      ],
      "metadata": {
        "id": "SBYratdI1dTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(tool_data,query_data,test_query,llm,temperature,sys_start,sys_end,usr_start,usr_end,ast_start):\n",
        "  return llm(retrieve_prompt_with_model(tool_data,query_data,test_query,sys_start,sys_end,usr_start,usr_end,ast_start))"
      ],
      "metadata": {
        "id": "5eQlKqWn6MXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_1 = \"Summarize my P1 issues in triage\"\n",
        "query_2 = \"Prioritize tasks similar to deve/0:issue/5\"\n",
        "query_3 = \"Add my blocker tickets to the current sprint\"\n",
        "query_4 = \"Prioritize low tickets that need response\"\n",
        "query_5 = \"Get all issues with priority p0 or p1 assigned to me and summarize them\"\n",
        "query_6 = \"List all tickets from github that need a response and add them to the current sprint\"\n",
        "query_7 = \"Get tasks assigned to me in triage, summarize them and create issues from that\"\n",
        "query_8 = \"List high severity github tickets from customer ABC, summarize them and prioritize the summary\"\n",
        "query_9 = \"Get all blocker tickets assigned to me, summarize and create issues from summary\"\n",
        "query_10 = \"List issues assigned to John, summarize them and prioritize\"\n",
        "query_11 = \"List high priority tickets from Acme Corp, summarize them\"\n",
        "query_12 = \"Add my actionable tasks from meeting notes M to my current sprint\"\n",
        "query_13 = \"What are similar issues to issue ISSUE-1?\"\n",
        "query_14 = \"Summarize my tickets needing response\"\n",
        "query_15 = \"Get me all the blocker tickets from github and slack channels under parts ENH-123, PROD-123, CAPL-123, CAPL-359 that need a response, prioritize them and add them to the current sprint\"\n",
        "query_16 = \"What are all my p0, p1 priority issues under parts HGH-262, FEAT-007 in the design stage? Summarize them and add them to the current sprint\"\n",
        "query_17 = \"Get all issues assigned to current user that are in design stage under parts FEAT-123 and FEAT-456. Filter them by priority p1 and p2. Summarize them and add them to current sprint.\"\n",
        "query_18 = \"Get all tasks assigned to me. Filter tasks in progress stage. Summarize the tasks. Get similar tasks to the top 3 tasks from the summary and add them to the current sprint\"\n",
        "query_19 = \"Search for user DEV-123, prioritize and add their P0 issues to the current sprint\"\n",
        "query_20 = \"Get the status of a build from last night\"\n",
        "query_21 = \"Schedule a meeting with the stakeholders\"\n",
        "\n",
        "queries = [\n",
        "    query_1, query_2, query_3, query_4, query_5, query_6, query_7, query_8, query_9,\n",
        "    query_10, query_11, query_12, query_13, query_14, query_15, query_16, query_17,\n",
        "    query_18, query_19, query_20, query_21\n",
        "]\n"
      ],
      "metadata": {
        "id": "jVFKZt9Z46VG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Zephyr"
      ],
      "metadata": {
        "id": "VJWBdG5DvkxS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y73ZS1m4vjzH"
      },
      "outputs": [],
      "source": [
        "zephyr = AutoModelForCausalLM.from_pretrained(\"TheBloke/zephyr-7B-beta-GGUF\", model_file=\"zephyr-7b-beta.Q4_K_M.gguf\", model_type='mistral',gpu_layers=400,context_length=8192,max_new_tokens=2048,temperature=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response_zephyr=[]\n",
        "for query in tqdm.tqdm(queries):\n",
        "  a=predict(tool_data,query_data,query,zephyr,0.5,'<|system|>','</s>','<|user|>','</s>','<|assistant|>')\n",
        "  response_zephyr.append({'query':query,'answer':a})"
      ],
      "metadata": {
        "id": "M3TN_1KY41CV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response_zephyr"
      ],
      "metadata": {
        "id": "OIwlyvnHCnMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Neural chat"
      ],
      "metadata": {
        "id": "pltxXeSVAzEo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "neural_chat = AutoModelForCausalLM.from_pretrained(\"TheBloke/neural-chat-7B-v3-1-GGUF\", model_file=\"neural-chat-7b-v3-1.Q4_K_M.gguf\", model_type=\"mistral\",gpu_layers=400,context_length=8192,max_new_tokens=2048,temperature=0.5)"
      ],
      "metadata": {
        "id": "6vvby5nKEkWn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ans=[]\n",
        "for inp in tqdm.tqdm(queries):\n",
        "\n",
        "  a=predict(tool_data,query_data,inp,neural_chat,0.5,'### System:',' ',' ','### User:','### Assistant:')\n",
        "  ans.append({'query':query,'answer':a})"
      ],
      "metadata": {
        "id": "BanKogP4Etyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ans"
      ],
      "metadata": {
        "id": "5VJL9XmJFXV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Mistral"
      ],
      "metadata": {
        "id": "Vqq92AZxFY3u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mistral = AutoModelForCausalLM.from_pretrained(\"TheBloke/Mistral-7B-Instruct-v0.1-GGUF\", model_file=\"mistral-7b-instruct-v0.1.Q4_K_M.gguf\", model_type=\"mistral\",gpu_layers=900,context_length=8192,max_new_tokens=2048,temperature=0.5)"
      ],
      "metadata": {
        "id": "i7ta4swoF9zJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response_mistral=[]\n",
        "for query in tqdm.tqdm(queries):\n",
        "  print(query)\n",
        "  a=predict(tool_data,query_data,query,mistral,0.5,'<s>[INST]',' ',' ','[/INST}</s>','')\n",
        "  response_mistral.append({'query':query,'answer':a})"
      ],
      "metadata": {
        "id": "GwQ484uuFdU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response_mistral"
      ],
      "metadata": {
        "id": "WAjhiZyGF_My"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural Hermes"
      ],
      "metadata": {
        "id": "J2gffNL3GA_-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "neural_hermes = AutoModelForCausalLM.from_pretrained(\"TheBloke/NeuralHermes-2.5-Mistral-7B-GGUF\",model_file = \"neuralhermes-2.5-mistral-7b.Q5_K_M.gguf\", gpu_layers=400,context_length=4096)"
      ],
      "metadata": {
        "id": "_B_Ydn3DGMfp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ans=[]\n",
        "for inp in tqdm.tqdm(queries):\n",
        "  a=predict(tool_data,query_data,inp,neural_hermes,0.5,'<|im_start|>system','<|im_end|>','<|im_start|>user','<|im_end|>','<|im_start>asisstant')\n",
        "  ans.append({'query':query,'answer':a})"
      ],
      "metadata": {
        "id": "AkXHvwCNG30F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inp"
      ],
      "metadata": {
        "id": "1D2NBkIbrdVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ans"
      ],
      "metadata": {
        "id": "fSqVWmJAHKuG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LLama 7B"
      ],
      "metadata": {
        "id": "qxyQVHa0HMfB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llama = AutoModelForCausalLM.from_pretrained(\"TheBloke/Llama-2-7B-GGUF\", model_file=\"llama-2-7b.Q4_K_M.gguf\", model_type=\"llama\",gpu_layers=400,context_length=8192,max_new_tokens=2048,temperature=0.5)"
      ],
      "metadata": {
        "id": "FPhQVzzTHR5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ans=[]\n",
        "for inp in tqdm.tqdm(queries):\n",
        "  a=predict(tool_data,query_data,inp,llama,0.5,' ',' ',' ',' ','')\n",
        "  ans.append({'query':inp,'answer':a})"
      ],
      "metadata": {
        "id": "_MsdENnTHYr2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ans"
      ],
      "metadata": {
        "id": "jglv50CPHkIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##cybertron una"
      ],
      "metadata": {
        "id": "8WdSH9NA6DvC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U bitsandbytes\n",
        "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
        "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
        "!pip install -q -U git+https://github.com/huggingface/accelerate.git"
      ],
      "metadata": {
        "id": "wfOyper56C3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --no-cache-dir transformers sentencepiece"
      ],
      "metadata": {
        "id": "74zHJQlw6OOe"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "f7q7Oj5tkKDD",
        "EuRf49APkYhr",
        "VOcJg5_mkrf_",
        "uv0yRBHlly5F",
        "0qjLJfNPWs2E",
        "T_Q0C-qNacLA",
        "ixWjSbjP323L",
        "iKrVOySz6Eyq",
        "4kVfb62I6GC6",
        "Wj3cDeQk4RTF",
        "fHSss0yFJXRM",
        "CEhLdgIAnp9B",
        "uqCHPXsN7Yzu",
        "L265_gqUSPAH",
        "SYAuZJUfAp98",
        "VJWBdG5DvkxS",
        "pltxXeSVAzEo"
      ],
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}