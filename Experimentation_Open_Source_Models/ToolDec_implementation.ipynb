{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "WjeK4TCrHYVj",
        "apQVcVzNHag4",
        "rj2lyc6xWH-e",
        "g13K_pWkCS3q"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##HOW TO RUN"
      ],
      "metadata": {
        "id": "tRjWjnMbMuTe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the 3 sections\n",
        "  - Importing and loading the data\n",
        "  - Loading the model\n",
        "  - Functions\n",
        "   \n",
        "   \n",
        "Provide the custom query in the Inference section and use predict_answer to get the solution"
      ],
      "metadata": {
        "id": "vaLUauE9J2IJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing and loading the data"
      ],
      "metadata": {
        "id": "WjeK4TCrHYVj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers ctransformers[cuda] sentence-transformers faiss-cpu accelerate bitsandbytes -q"
      ],
      "metadata": {
        "id": "ZgGyWYanlvr_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6291b33-a7b2-4ba3-92dc-2b9361a9b8cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m63.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.7/265.7 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import json\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer"
      ],
      "metadata": {
        "id": "-Sfma4N9BQYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O tools.json \"https://drive.usercontent.google.com/download?id=1RSC_94KKmvtWnI15RVZkLCy4eTVhxGU_&export=download&authuser=0&confirm=t&uuid=394a0e2f-bb02-4027-8224-ef451c182f49&at=APZUnTVfd2focJ6u3DYRiDeKWBUr:1702403341998\"\n",
        "!wget -O examples.json \"https://drive.usercontent.google.com/download?id=1n9eede9tNiqfPfWyc3lx1AHgqJgxLFQ5&export=download&authuser=0&confirm=t&uuid=eabc4504-aca7-4dfc-a6d9-52d357149ee6&at=APZUnTWmVVzr0utprDNGk8VXTU6f:1702403392202\"\n",
        "!wget -O claude_examples_ps_tools_all.json \"https://drive.usercontent.google.com/download?id=17orzUW3_n31pZFyL0F-ekGH5iKlh2SUX&export=download&authuser=0&confirm=t&uuid=61097f63-24fe-4c51-9710-77d7fd204c2b&at=APZUnTXUxVA_067GuWmlaIrm74Io:1702403397617\"\n",
        "with open('/content/examples.json', 'r') as f:\n",
        "    query_data = json.load(f)\n",
        "\n",
        "with open('/content/tools.json', 'r') as f:\n",
        "    tool_data = json.load(f)\n",
        "\n",
        "with open('/content/claude_examples_ps_tools_all.json', 'r') as f:\n",
        "    tool_query_data = json.load(f)"
      ],
      "metadata": {
        "id": "FpDrGwVdruFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading the model"
      ],
      "metadata": {
        "id": "apQVcVzNHag4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_name = \"HuggingFaceH4/zephyr-7b-beta\"\n",
        "prompt = \"Tell me about gravity\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", load_in_4bit=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "model_inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda:0\")\n",
        "\n",
        "output = model.generate(**model_inputs)\n",
        "\n",
        "print(tokenizer.decode(output[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "N-QddBOWGq49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Functions"
      ],
      "metadata": {
        "id": "rj2lyc6xWH-e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def simple_prompt(tool_data,query_data,query):\n",
        "  prompt_template=f\"\"\"\n",
        "  You are given the following TOOLS with their DESCRIPTIONS, ARGUMENTS to the tool in question with their DESCRIPTIONS.\n",
        "  {tool_data}.\n",
        "   To reference the value of the ith tool in the chain, use $$PREV[i] as argument value, where i refers to the output of the tool positioned in the ith place in the JSON.\n",
        "   EXAMPLE QUERY 1: {query_data[0]['query']}\n",
        "   ANSWER: {query_data[0]['answer']}\n",
        "   EXAMPLE QUERY 2: {query_data[3]['query']}\n",
        "   ANSWER: {query_data[3]['answer']}\n",
        "   EXAMPLE QUERY 3: {query_data[2]['query']}\n",
        "   ANSWER: {query_data[2]['answer']}\n",
        "  The above are solutions to some queries.\n",
        "  It is your task to answer the following QUERY. Think step by step. ONLY OUTPUT THE ANSWER TO THE QUERY AND THE ANSWER SHOULD BE IN JSON FORMAT. DO NOT DEVIATE from the instructions and keep in mind that the output answer should be in correct json format.\n",
        "   QUERY: {query}.\"\"\"\n",
        "  return prompt_template"
      ],
      "metadata": {
        "id": "xRUDfbU_mglZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prompt_template(query):\n",
        "  return f\"\"\"<|system|>\n",
        "You are a highly intelligent AI assistant that can help users with questions that can be answered by the use of a set of TOOLS. You specialize in identifying tools with arguments to solve the query.\n",
        "<|user|>\n",
        "{simple_prompt(tool_data,query_data,query)}\n",
        "<|assistant|>\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "hmr1tYoalcnk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Python program for insert and search\n",
        "# operation in a Trie\n",
        "import numpy as np\n",
        "\n",
        "class TrieNode:\n",
        "    def __init__(self):\n",
        "        self.children = [None]*len(tokenizer.vocab)\n",
        "\n",
        "        # isEndOfWord is True if node represent the end of the word\n",
        "        self.isEndOfWord = False\n",
        "\n",
        "class Trie:\n",
        "\n",
        "    # Trie data structure class\n",
        "    def __init__(self):\n",
        "        self.root = self.getNode()\n",
        "\n",
        "    def getNode(self):\n",
        "\n",
        "        return TrieNode()\n",
        "\n",
        "    def _tokToIndex(self,ch):\n",
        "\n",
        "        return ch\n",
        "\n",
        "\n",
        "    def insert(self,key):\n",
        "\n",
        "        pCrawl = self.root\n",
        "        tokens = tokenizer(key)['input_ids'][1:]\n",
        "\n",
        "        for token in tokens:\n",
        "          if not pCrawl.children[token]:\n",
        "            pCrawl.children[token] = self.getNode()\n",
        "          pCrawl = pCrawl.children[token]\n",
        "\n",
        "        pCrawl.isEndOfWord = True\n",
        "\n",
        "    def get(self, key):\n",
        "        pCrawl = self.root\n",
        "        tokens = tokenizer(key)['input_ids'][1:]\n",
        "        for token in tokens:\n",
        "          if not pCrawl.children[token]:\n",
        "            return False\n",
        "          pCrawl=pCrawl.children[token]\n",
        "\n",
        "        indexes=np.argwhere(np.array(pCrawl.children)).flatten()\n",
        "        l=[]\n",
        "        for index in indexes:\n",
        "          l.append(tokenizer.decode(index))\n",
        "        return l"
      ],
      "metadata": {
        "id": "BkHPfpWN_cf2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tool_name_trie = Trie()\n",
        "for tool in tool_data['tools']:\n",
        "  tool_name_trie.insert(tool['tool_name'])"
      ],
      "metadata": {
        "id": "mWwndaqzcd3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "argument_name_trie={}\n",
        "for tool in tool_data[\"tools\"]:\n",
        "  argument_name_trie[tool[\"tool_name\"]]=Trie()\n",
        "  for argument in tool['argument_list']:\n",
        "    argument_name_trie[tool[\"tool_name\"]].insert(argument['argument_name'])"
      ],
      "metadata": {
        "id": "ML0bFlWRdzWR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(input, next_tokens = None):\n",
        "  torch.cuda.empty_cache()\n",
        "  model_inputs=input\n",
        "  if next_tokens:\n",
        "    logits =  model.generate(input_ids = model_inputs.input_ids, attention_mask = model_inputs.attention_mask, max_new_tokens = 1, output_scores = True, return_dict_in_generate=True).scores[0][0].cpu()\n",
        "    return torch.argmax(logits[next_tokens], dim = -1).cpu()\n",
        "  return tokenizer.decode(model.generate(input_ids = model_inputs.input_ids, attention_mask = model_inputs.attention_mask).cpu()[0])"
      ],
      "metadata": {
        "id": "71oQfn5sJzVe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(input, next_tokens = None):\n",
        "  torch.cuda.empty_cache()\n",
        "  model_inputs=input\n",
        "  if next_tokens:\n",
        "    def restrict_decode_vocab(batch_id, input_id):\n",
        "      return next_tokens\n",
        "    logits =  model.generate(max_new_tokens=1, input_ids = model_inputs.input_ids, attention_mask = model_inputs.attention_mask, prefix_allowed_tokens_fn=restrict_decode_vocab, do_sample = True, temperature = 0.4).cpu()\n",
        "    model_pred=next_tokens.index(logits.flatten()[-1])\n",
        "    return model_pred\n",
        "  # return model.generate(input_ids = model_inputs.input_ids, attention_mask = model_inputs.attention_mask, max_new_tokens = 30,stopping_criteria = StoppingCriteriaList([DummyStopCriterion()]), do_sample = True, temperature = 0.4).cpu()[0][len(model_inputs.input_ids[0]):]\n",
        "  return model.generate(input_ids = model_inputs.input_ids, attention_mask = model_inputs.attention_mask, max_new_tokens = 30,eos_token_id=28739, do_sample = True, temperature = 0.4).cpu()[0][len(model_inputs.input_ids[0]):]"
      ],
      "metadata": {
        "id": "M4OPcFZPsthT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add(model_inputs,t1,t2):\n",
        "  model_inputs[\"input_ids\"]=torch.concat((model_inputs.input_ids,torch.tensor([t1]).to(\"cuda\")),dim=1)\n",
        "  model_inputs[\"attention_mask\"]=torch.concat((model_inputs.attention_mask,torch.tensor([t2]).to(\"cuda\")),dim=1)\n",
        "  return model_inputs"
      ],
      "metadata": {
        "id": "65pUh2BMMfN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decode(prompt):\n",
        "  state = \"START\"\n",
        "  input = prompt\n",
        "  model_inputs = tokenizer(input, return_tensors=\"pt\").to(\"cuda:0\")\n",
        "  while True:\n",
        "    torch.cuda.empty_cache()\n",
        "    print(input[len(prompt):])\n",
        "    match state:\n",
        "      case \"START\":\n",
        "        # this state starts the answer\n",
        "        input += '['                                         # adding the answer to the input\n",
        "        model_inputs=add(model_inputs,[733],[1])             # add function adds the given list to the model_inputs\n",
        "        options = [\"]\", '{\"tool_name\":\"']                    # these are the options LLM can choose\n",
        "        next_tokens = []\n",
        "        for op in options:\n",
        "          next_tokens.append(tokenizer.encode(op)[1])        # tokenizing the options and only selecting the first token\n",
        "        model_pred = predict(model_inputs, next_tokens)      # obtaining the prediction from the LLM\n",
        "        if model_pred == 0:                                  # changing the state according to the LLM prediction\n",
        "          state = \"END\"                                      # going to END state if the model prediction is '['\n",
        "        else:\n",
        "          state = \"NT\"                                       # going to NT state if the model prediction is '{\"tool_name\":\"'\n",
        "      case \"END\":\n",
        "        # this state ends the answer\n",
        "        input += ']'\n",
        "        model_inputs=add(model_inputs,[4709],[1])\n",
        "        break\n",
        "      case \"NT\":\n",
        "        # this state starts adding new tool\n",
        "        input+='{\"tool_name\":\"'\n",
        "        model_inputs=add(model_inputs,[9830, 6462, 28730, 861, 10549],[1,1,1,1,1])\n",
        "        options=tool_name_trie.get(\"\")                       # gets the first token of all the tools\n",
        "        next_tokens = []\n",
        "        for op in options:\n",
        "          next_tokens.append(tokenizer.encode(op)[1])\n",
        "        model_pred = predict(model_inputs, next_tokens)\n",
        "        state=\"CT\"                                           # changing the state\n",
        "        next_tool_token=options[model_pred]\n",
        "        tool_name = next_tool_token                          # storing the tool name for future use\n",
        "      case \"CT\":\n",
        "        # this state continue adding the tool name to the answer\n",
        "        input+=next_tool_token\n",
        "        t=[tokenizer(next_tool_token).input_ids[1]]\n",
        "        model_inputs=add(model_inputs,t,[1])\n",
        "        options=tool_name_trie.get(tool_name) + ['\"']       # gives the continuation token for the tool\n",
        "        flag = False                                        # to check if we need to be in the same state or to continue the while loop to change the state\n",
        "        while(len(options) == 2):                           # autofills the tool name if there is no other tool starting with the next_tool_token\n",
        "          next_tool_token=options[0]\n",
        "          input+=next_tool_token\n",
        "          tool_name += next_tool_token\n",
        "          t=[tokenizer(next_tool_token).input_ids[1]]\n",
        "          model_inputs=add(model_inputs,t,[1])\n",
        "          temp = tool_name_trie.get(tool_name)\n",
        "          if temp:                                          # continues the auto-fill if there are continuation tokens to the existing tool name\n",
        "            options= temp + ['\"']\n",
        "          else:                                             # else changes the state if the tool name gets completed\n",
        "            state = \"SAL\"\n",
        "            flag = True\n",
        "            break\n",
        "        if flag:\n",
        "          flag = False\n",
        "          continue\n",
        "        next_tokens = []\n",
        "        for op in options:\n",
        "          next_tokens.append(tokenizer.encode(op)[1])\n",
        "        model_pred = predict(model_inputs, next_tokens)\n",
        "        if options[model_pred]=='\"':                        # changing the state according to the LLM prediction\n",
        "          state=\"SAL\"\n",
        "        else:\n",
        "          state=\"CT\"\n",
        "          next_tool_token=options[model_pred]\n",
        "          tool_name += next_tool_token                      # adding the new token to the tool name\n",
        "      case \"SAL\":\n",
        "        # this state starts the argument list in the respective tool\n",
        "        input+=',\"arguments\":['\n",
        "        model_inputs=add(model_inputs,[28705, 862, 16684, 1264, 28792],[1,1,1,1,1])\n",
        "        options=[']','{\"argument_name\":\"']\n",
        "        next_tokens = []\n",
        "        for op in options:\n",
        "          next_tokens.append(tokenizer.encode(op)[1])\n",
        "        model_pred = predict(model_inputs, next_tokens)\n",
        "        if model_pred==0:                                   # changing the state according to the LLM prediction\n",
        "          state=\"EAL\"\n",
        "        else:\n",
        "          state=\"CAN\"\n",
        "      case \"EAL\":\n",
        "        # this state ends the argument list\n",
        "        input+=']}'\n",
        "        model_inputs=add(model_inputs,[4709, 28752],[1,1])\n",
        "        options=[',',']']\n",
        "        next_tokens = []\n",
        "        for op in options:\n",
        "          next_tokens.append(tokenizer.encode(op)[1])\n",
        "        model_pred = predict(model_inputs, next_tokens)\n",
        "        if model_pred==1:                                   # changing the state according to the LLM prediction\n",
        "          state=\"END\"\n",
        "        else:\n",
        "          state=\"CTL\"\n",
        "      case \"CTL\":\n",
        "        # this state continue adding new tools to the existing tool list\n",
        "        input+=','\n",
        "        model_inputs=add(model_inputs,[1200],[1])\n",
        "        state=\"NT\"\n",
        "      case \"CAN\":\n",
        "        # this state start creating the argument name\n",
        "        input+='\"{argument_name\":\"'\n",
        "        model_inputs=add(model_inputs,[25002, 14635, 28730, 861, 10549],[1,1,1,1,1])\n",
        "        arg_name = \"\"                                       # storing the argument name for the future usage\n",
        "        options=argument_name_trie[tool_name].get(\"\")       # gets the first token of all the arguments\n",
        "        next_tokens = []\n",
        "        for op in options:\n",
        "          next_tokens.append(tokenizer.encode(op)[1])\n",
        "        model_pred = predict(model_inputs, next_tokens)\n",
        "        if options[model_pred]=='\"':                        # changing the state according to the LLM prediction\n",
        "          state=\"AV\"\n",
        "        else:\n",
        "          state=\"CA\"\n",
        "          next_argument_token=options[model_pred]\n",
        "          arg_name += next_argument_token\n",
        "      case \"CA\":\n",
        "        # this state continue creating the argument name\n",
        "        input+=next_argument_token\n",
        "        t=[tokenizer(next_argument_token).input_ids[1]]\n",
        "        model_inputs=add(model_inputs,t,[1])\n",
        "        options=argument_name_trie[tool_name].get(arg_name) + ['\"'] # gives the continuation token for the argument name\n",
        "\n",
        "        flag = False                            # to check if we need to be in the same state or to continue the while loop to change the state\n",
        "        while(len(options) == 2):               # autofills the argument name if there is no other argument name starting with the next_argument_token\n",
        "          next_argument_token=options[0]\n",
        "          input+=next_argument_token\n",
        "          arg_name += next_argument_token\n",
        "          t=[tokenizer(next_argument_token).input_ids[1]]\n",
        "          model_inputs=add(model_inputs,t,[1])\n",
        "          temp = argument_name_trie[tool_name].get(arg_name)\n",
        "          if temp:\n",
        "            options= temp + ['\"']\n",
        "          else:\n",
        "            state = \"AV\"\n",
        "            flag = True\n",
        "            break\n",
        "        if flag:\n",
        "          flag = False\n",
        "          continue\n",
        "\n",
        "        next_tokens = []\n",
        "        for op in options:\n",
        "          next_tokens.append(tokenizer.encode(op)[1])\n",
        "        model_pred = predict(model_inputs, next_tokens)\n",
        "        if options[model_pred]=='\"':          # changing the state according to the LLM prediction\n",
        "          state=\"AV\"\n",
        "        else:\n",
        "          state=\"CA\"\n",
        "          next_argument_token=options[model_pred]\n",
        "          arg_name += next_argument_token\n",
        "      case \"AV\":\n",
        "        # this state starts adding the argument value to the answer\n",
        "        input+=',\"argument_value\":'\n",
        "        model_inputs=add(model_inputs,[28705, 862, 14635, 28730, 1431, 1264],[1,1,1,1,1,1])\n",
        "        model_pred = predict(model_inputs)\n",
        "        next_argument_value_token=tokenizer.decode(model_pred, skip_special_tokens = True)\n",
        "        state=\"CAV\"\n",
        "      case \"CAV\":\n",
        "        # this state continue adding the argument value to the answer\n",
        "        input+=next_argument_value_token\n",
        "        t=model_pred.tolist()\n",
        "        model_inputs=add(model_inputs,t,[1 for i in range(len(t))])\n",
        "        state=\"EAV\"\n",
        "      case \"EAV\":\n",
        "        # this state ends the argument value\n",
        "        input+='\"}'\n",
        "        model_inputs=add(model_inputs,[ 345, 28752],[1,1])\n",
        "        model_pred = predict(model_inputs)\n",
        "        next_argument_value_token=tokenizer.decode(model_pred, skip_special_tokens = True)\n",
        "        if next_argument_value_token==\",\":    # changing the state according to the LLM prediction\n",
        "          state=\"CAL\"\n",
        "        else:\n",
        "          state=\"EAL\"\n",
        "      case \"CAL\":\n",
        "        # this state continue adding new arguments to the existing argument list\n",
        "        input+=','\n",
        "        model_inputs=add(model_inputs,[1200],[1])\n",
        "        state=\"CAN\"\n",
        "  return input[len(prompt):]"
      ],
      "metadata": {
        "id": "x-LM9uqqGhGr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_answer(query):\n",
        "    return decode(prompt_template(query))"
      ],
      "metadata": {
        "id": "Nszhh5HPLmzz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#INFERENCE"
      ],
      "metadata": {
        "id": "g13K_pWkCS3q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query=\"Summarize my P1 issues in triage\""
      ],
      "metadata": {
        "id": "82UJ57LmIDrG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer = predict_answer(query)"
      ],
      "metadata": {
        "id": "VjtP3LdGhzPn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}